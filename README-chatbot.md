
# AI-Powered Chatbot - Setup Guide

## ğŸ“‹ Overview

This feature adds a local AI-powered chatbot to DocDataApp with **two modes**:

1. **Simple Chat Mode**: General AI conversation using LLM models (Gemma2, Llama, or Qwen)
2. **Document Assistant Mode** â­: Advanced document-aware chat with LOCAL_DOCS integration, retrieval-first approach, and deterministic numeric extraction

The Document Assistant mode uses **LLM models** for natural language understanding and **LOCAL_DOCS** (localStorage-based document storage) for semantic search. The system performs **deterministic numeric extraction** and **backend aggregation** to ensure accurate financial calculations.

### ğŸ¤– Supported LLM Models

| Model | Size | RAM | Context | Turkish | Best For |
|-------|------|-----|---------|---------|----------|
| **Gemma2:2b** â­ | 2B | ~1.5GB | 8K | â­â­â­â­ | Fast & Efficient (Recommended) |
| **Llama 3.2:3b** | 3B | ~2GB | 8K | â­â­â­ | Quick responses |
| **Qwen 2.5:7b** | 7B | ~4.5GB | 128K | â­â­â­â­ | Best overall |

> **ğŸ’¡ Recommendation**: Use **Gemma2:2b** for fast inference, low memory usage, and good Turkish language support.

> **ğŸ“¦ Model Info**: Gemma2 is Google's efficient open model optimized for fast inference with excellent multilingual support including Turkish.

## ğŸ—ï¸ Architecture

### Simple Chat Mode

```
User Query â†’ LLM (Gemma2/Llama) â†’ Response
```

### Document Assistant Mode (Production-Ready)

```
User Query
    â†“
1. LOCAL_DOCS Retrieval (Keyword + N-gram + Semantic)
    â†“
2. Relevance Scoring (0.0-1.0) â†’ Filter (threshold: 0.4)
    â†“
3. Numeric Extraction (Turkish locale-aware regex)
    â†“
4. Aggregate Computation (Sum, Avg, Median, Count)
    â†“
5. LLM Processing (Gemma2) â†’ Natural language answer + __meta__ JSON
    â†“
6. Response + Metadata (references, numeric values, statistics)
```

### Key Features

**Document Assistant Mode:**
- âœ… **LOCAL_DOCS Integration**: Direct access to localStorage documents (no external DB required)
- âœ… **Retrieval-First Approach**: Keyword â†’ Partial â†’ N-gram â†’ Semantic matching
- âœ… **Deterministic Extraction**: Turkish-locale numeric extraction (1.234,56 TL format)
- âœ… **Backend Calculations**: Sum, average, median, min/max computed deterministically
- âœ… **Structured Output**: __meta__ JSON block with references, numeric values, and statistics
- âœ… **Anomaly Detection**: Contradictions and data quality verification
- âœ… **Turkish Language Optimized**: TDK-compliant, natural Turkish responses
- âœ… **Configurable Options**: compute, showRaw, maxRefs, locale
- âœ… **Low Temperature**: Deterministic responses (temp: 0.15)

**General Features:**
- âœ… **Dual Mode Support**: Switch between simple chat and document assistant
- âœ… **Conversation Memory**: Context-aware (last 5-10 messages)
- âœ… **Fallback Mode**: Works even if LLM connection fails
- âœ… **Turkish Locale Support**: Complete Turkish text normalization

## ğŸš€ Quick Start

### Prerequisites

- Node.js >= 20
- Ollama (required for LLM models)
- Processed documents in localStorage (for Document Assistant mode)

### 1. Install Dependencies

```bash
npm install
```

### 2. Start BGE-M3 Embedding Server

```bash
cd model_server
pip install -r requirements.txt
python app.py
```

Server will start at `http://127.0.0.1:7860`

### 3. Start LLM with Ollama

#### Option A: Llama 3.2:3b (Lightweight)

```bash
# Install Ollama: https://ollama.ai/download
# Pull and run Llama 3.2:3b
ollama run llama3.2:3b
```

**Specs:**
- 3B parameters
- 8K context window
- ~2GB RAM
- Good for quick responses

#### Option B: Gemma2 2b (Recommended - Fast & Efficient)

```bash
# Pull and run Gemma2 2b
ollama pull gemma2:2b
ollama run gemma2:2b
```

**Specs:**
- 4B parameters
- 128K context window (16x larger!)
- ~4GB RAM (quantized version)
- Excellent Turkish language support
- 140+ languages
- Better for complex analysis
- Latest Gemma2 model with fast inference

**Optimized Parameters:**
- `temperature`: 0.25 (doÄŸruluk odaklÄ±)
- `top_p`: 0.9
- `repeat_penalty`: 1.1
- `context_length`: 128K
- `quantization`: q4_k_m (hÄ±z/kalite dengesi)

#### Option C: Qwen 2.5:7b (Best Performance)

```bash
# Pull and run Qwen 2.5:7b
ollama run qwen2.5:7b
```

**Specs:**
- 7B parameters
- 128K context window
- ~4.5GB RAM
- Best overall performance
- Excellent multilingual support

Ollama API will be at `http://127.0.0.1:11434`

#### ğŸ› ï¸ CPU Mode (CUDA Error Fix)

If you encounter **CUDA errors** or GPU issues:

**Option 1: Use Batch File (Easiest)**
```bash
# Double-click this file to start Ollama in CPU mode
start_ollama_cpu.bat
```

**Option 2: Manual Environment Variable**
```bash
# PowerShell
$env:OLLAMA_NUM_GPU=0
ollama serve

# CMD
set OLLAMA_NUM_GPU=0
ollama serve
```

**Option 3: Permanent Fix**
Add to your `.env`:
```bash
OLLAMA_NUM_GPU=0
```

> **Note**: CPU mode is slower (~2-3x) but more stable and doesn't require GPU.

### 4. Configure Environment Variables

Create or update `.env`:

```bash
# BGE-M3 Embedding Server
MODEL_SERVER_URL=http://127.0.0.1:7860

# Llama LLM Server
LLAMA_SERVER_URL=http://127.0.0.1:11434

# LLM Model Selection (choose one)
# Options: gemma2:2b, llama3.2:3b, qwen2.5:7b
LLAMA_MODEL=gemma2:2b

# Vector Database (pgvector or qdrant)
VECTOR_DB=pgvector

# Supabase (if using pgvector)
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key

# OR Qdrant (if using qdrant)
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=your-api-key

# Search settings
TOP_K=50
```

### 5. Setup Vector Database

#### Option A: Supabase pgvector

1. Enable pgvector extension in Supabase:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

2. Create the `match_embeddings` function:

```sql
CREATE OR REPLACE FUNCTION match_embeddings(
  query_embedding VECTOR(1024),
  match_threshold FLOAT,
  match_count INT
)
RETURNS TABLE (
  section_id TEXT,
  document_id TEXT,
  filename TEXT,
  content TEXT,
  page_number INT,
  section_title TEXT,
  content_type TEXT,
  similarity FLOAT
)
LANGUAGE plpgsql
AS $$
BEGIN
  RETURN QUERY
  SELECT
    ts.id AS section_id,
    ts.document_id,
    d.filename,
    ts.content,
    ts.page_number,
    ts.section_title,
    ts.content_type,
    1 - (e.embedding <=> query_embedding) AS similarity
  FROM embeddings e
  JOIN text_sections ts ON ts.id = e.text_section_id
  JOIN documents d ON d.id = ts.document_id
  WHERE 1 - (e.embedding <=> query_embedding) > match_threshold
  ORDER BY e.embedding <=> query_embedding
  LIMIT match_count;
END;
$$;
```

#### Option B: Qdrant

```bash
# Start Qdrant with Docker
docker run -p 6333:6333 qdrant/qdrant
```

Create collection via Qdrant API or UI.

### 6. Configure GPU/CPU Mode

GPU ve CPU modlarÄ± arasÄ±nda geÃ§iÅŸ yapmak iÃ§in:

**Option A: UI ile (Ã–nerilen)**

1. UygulamayÄ± Ã§alÄ±ÅŸtÄ±rÄ±n: `npm run dev`
2. **Settings** sayfasÄ±na gidin
3. **AI & Performans AyarlarÄ±** bÃ¶lÃ¼mÃ¼nde:
   - **GPU HÄ±zlandÄ±rma** toggle'Ä±nÄ± aÃ§Ä±n/kapatÄ±n
   - **GPU Warmup** ile uygulama baÅŸlangÄ±cÄ±nda Ä±sÄ±nmayÄ± aktifleÅŸtirin
   - **Maksimum Context UzunluÄŸu** ile performans/kalite dengesini ayarlayÄ±n

**Option B: Environment Variables ile**

`.env` dosyasÄ±nda:

```bash
# GPU modu (auto, enabled, disabled)
GPU_MODE=auto

# GPU warmup
GPU_WARMUP=true

# Max context length
MAX_CONTEXT_LENGTH=15000
```

### 7. Run the Application

```bash
npm run dev
```

Navigate to the AI Chat page or Settings page in the application.

## ğŸ›ï¸ GPU/CPU Toggle KullanÄ±mÄ±

### Settings SayfasÄ±nda GPU KontrolÃ¼

GPU ve CPU modlarÄ± arasÄ±nda kolayca geÃ§iÅŸ yapabilirsiniz:

```
Settings > AI & Performans AyarlarÄ±
â”œâ”€â”€ GPU Durumu: KullanÄ±labilir âœ… (NVIDIA GeForce RTX...)
â”œâ”€â”€ GPU HÄ±zlandÄ±rma: [ON/OFF Toggle]
â”œâ”€â”€ GPU Warmup (Ã–n IsÄ±tma): [ON/OFF Toggle]
â””â”€â”€ Maksimum Context UzunluÄŸu: [4000-32000]
```

**GPU Durumu GÃ¶stergeleri:**
- ğŸŸ¢ **KullanÄ±labilir** - GPU algÄ±landÄ± ve kullanÄ±ma hazÄ±r
- ğŸŸ¡ **KullanÄ±lamÄ±yor** - GPU bulunamadÄ±, CPU modu Ã¶neriliyor

**Toggle States:**
- **GPU AÃ§Ä±k** ğŸš€ - GPU kullanÄ±lÄ±yor (2-3x daha hÄ±zlÄ±)
- **GPU KapalÄ±** ğŸ¢ - CPU kullanÄ±lÄ±yor (daha yavaÅŸ ama kararlÄ±)

**Performans Ä°puÃ§larÄ±:**
- GPU varsa her zaman aÃ§Ä±k tutun
- Warmup'Ä± aktif edin (uygulama baÅŸlangÄ±cÄ± biraz uzar ama ilk yanÄ±t Ã§ok hÄ±zlÄ± olur)
- Context uzunluÄŸunu sisteminize gÃ¶re ayarlayÄ±n:
  - CPU: 8000
  - GPU (4GB VRAM): 15000
  - GPU (8GB+ VRAM): 32000

### Runtime'da Mod DeÄŸiÅŸtirme

AyarlarÄ± deÄŸiÅŸtirdiÄŸinizde:
1. DeÄŸiÅŸiklik anÄ±nda kaydedilir
2. Yeni ayarlar bir sonraki AI sorgusundan itibaren geÃ§erli olur
3. Uygulama yeniden baÅŸlatÄ±lmasÄ±na gerek yoktur

## ğŸ§ª Testing

### Run Unit Tests

```bash
npm test
```

Tests cover:
- âœ… Turkish number parsing (1.234,56)
- âœ… US number parsing (1,234.56)
- âœ… Currency detection (TL, USD, EUR)
- âœ… Negative numbers
- âœ… Date extraction (DD.MM.YYYY, YYYY-MM-DD)
- âœ… Invoice ID patterns
- âœ… Aggregation (sum, avg, median, dedupe)

### Manual Testing

#### 1. Health Check

```bash
curl http://127.0.0.1:7860/health
curl http://127.0.0.1:11434/api/tags
```

#### 2. Test BGE-M3 Embedding

```bash
curl -X POST http://127.0.0.1:7860/embed \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Fatura toplam tutarÄ± 1.234,56 TL"],
    "normalize": true
  }'
```

Expected: 1024-dim vector array

#### 3. Test Llama Generation

```bash
curl -X POST http://127.0.0.1:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mistral",
    "prompt": "Merhaba, nasÄ±lsÄ±n?",
    "stream": false
  }'
```

Expected: Turkish response

#### 4. Test Chat Query (via Electron)

In the app, ask:
- "FaturalarÄ±mÄ±n toplam tutarÄ± nedir?"
- "KaÃ§ adet fatura var?"
- "Ortalama fatura tutarÄ± ne kadar?"

## ğŸ“Š Benchmark

### Latency Goals

- Embedding (1 query): < 100ms
- Retrieval (50 chunks): < 500ms
- Extraction + Aggregation: < 100ms
- Llama formatting: < 2s
- **Total: < 2.5s**

### Accuracy Goals

- Numeric extraction: 95%+ precision
- Date parsing: 95%+ recall
- Invoice ID detection: 90%+ F1

## ğŸ”§ Troubleshooting

### BGE-M3 Server Not Starting

```bash
# Check Python version
python --version  # Should be 3.9+

# Install dependencies
pip install torch transformers sentence-transformers flask flask-cors

# Try running with verbose output
python app.py --verbose
```

### Llama Server Issues

```bash
# Check Ollama is running
ollama list

# Restart Ollama
ollama serve

# Test model
ollama run mistral "Test"
```

### Vector Database Connection Failed

**Supabase:**
- Verify `SUPABASE_URL` and `SUPABASE_ANON_KEY`
- Check if pgvector extension is enabled
- Verify `match_embeddings` function exists

**Qdrant:**
- Check if Qdrant is running: `curl http://localhost:6333`
- Verify collection exists

### No Results from Query

1. Check if documents are indexed:
   - Verify embeddings in database
   - Check text_sections table

2. Lower similarity threshold in `.env`:
   ```bash
   SIMILARITY_THRESHOLD=0.1
   ```

3. Increase TOP_K:
   ```bash
   TOP_K=100
   ```

## ğŸ” Security Notes

- All processing is **100% local** (no data sent to cloud)
- Supabase connection uses anon key (RLS policies apply)
- Llama runs locally (no API keys needed)
- No PII is logged (only masked IDs in dev mode)

## ğŸ“ˆ Performance Optimization

### Caching

Enable caching for repeated queries:

```typescript
// In chatController.ts
private queryCache = new Map<string, ChatQueryResponse>();
```

### Batch Processing

Process multiple queries in parallel:

```bash
TOP_K=20  # Reduce for faster retrieval
```

### GPU Acceleration

#### BGE-M3 GPU KullanÄ±mÄ±

```python
# In app.py
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = SentenceTransformer('BAAI/bge-m3', device=device)
```

#### Gemma2/Llama GPU KullanÄ±mÄ±

Ollama otomatik olarak GPU'yu algÄ±lar:

```bash
# GPU ile Ã§alÄ±ÅŸtÄ±r (otomatik algÄ±lama)
ollama run gemma2:2b

# GPU kullanÄ±mÄ±nÄ± kontrol et
nvidia-smi
```

#### GPU Performans OptimizasyonlarÄ±

**1. Quantizasyon Seviyeleri**

Daha dÃ¼ÅŸÃ¼k quantizasyon â†’ daha hÄ±zlÄ± ve az bellek:

```bash
# INT8 quantization (Ã¶nerilen)
ollama pull gemma3:4b-it-q8_0

# INT4 quantization (en hÄ±zlÄ±, biraz kalite kaybÄ±)
ollama pull gemma3:4b-it-qat

# Tam precision (en yavaÅŸ, en iyi kalite)
ollama pull gemma3:4b-it
```

**Bellek KarÅŸÄ±laÅŸtÄ±rmasÄ±:**
- `q4_k_m`: ~2.5GB VRAM (Ã¶nerilen)
- `q8_0`: ~4GB VRAM
- `fp16`: ~8GB VRAM

**2. Context Window Optimizasyonu**

BÃ¼yÃ¼k context = yavaÅŸ iÅŸlem:

```typescript
// llamaClient.ts iÃ§inde
const maxQueryLength = 4000; // Gemma2 2B iÃ§in optimize (8K context)
const truncatedQuery = query.length > maxQueryLength 
  ? query.substring(0, maxQueryLength) + '\n\n[Mesaj kesildi]'
  : query;
```

**3. Streamed Output (Token AkÄ±ÅŸÄ±)**

GerÃ§ek zamanlÄ± yanÄ±t iÃ§in:

```typescript
// llamaClient.ts
const payload = {
  model: 'gemma2:2b',
  prompt: fullPrompt,
  stream: true, // Streaming etkinleÅŸtir
  options: {
    temperature: 0.25,
    num_predict: 300,
  },
};
```

**4. Warm-up Run**

Uygulama baÅŸlangÄ±cÄ±nda model Ä±sÄ±tma:

```typescript
// main.ts - uygulama baÅŸlangÄ±cÄ±nda
async function warmupGPU() {
  const llamaClient = new LlamaClient();
  
  console.log('ğŸ”¥ GPU warming up...');
  
  await llamaClient.generate({
    instructions: 'Merhaba',
    context: '',
    maxTokens: 10,
    temperature: 0.25,
  });
  
  console.log('âœ… GPU ready');
}

// App ready event'inde Ã§aÄŸÄ±r
app.whenReady().then(async () => {
  await warmupGPU();
  createWindow();
});
```

**5. Layer Cache (Katman Ã–nbellekleme)**

Ollama otomatik olarak GPU belleÄŸinde katman Ã¶nbelleklemesi yapar:

```bash
# Model GPU'ya yÃ¼klendiÄŸinde bellekte kalÄ±r
ollama run gemma2:2b

# Kapatmak iÃ§in:
ollama stop gemma2:2b
```

**6. Batch Processing (Paralel Ä°ÅŸleme)**

Birden fazla sorgu varsa:

```typescript
// chatController.ts
async handleBatchQueries(requests: ChatQueryRequest[]): Promise<ChatQueryResponse[]> {
  // Paralel iÅŸlem
  return await Promise.all(
    requests.map(req => this.handleChatQuery(req))
  );
}
```

#### DocDataApp GPU Entegrasyonu

**AdÄ±m 1: GPU KontrolÃ¼**

`.env` dosyasÄ±na:

```bash
# GPU modu (varsayÄ±lan: auto)
GPU_MODE=auto  # auto, enabled, disabled
OLLAMA_NUM_GPU=1  # 0 = CPU only, 1+ = GPU enabled
```

**AdÄ±m 2: Startup Script**

`start_ollama_gpu.bat` oluÅŸtur:

```bash
@echo off
echo Starting Ollama with GPU...

REM GPU kontrolÃ¼
nvidia-smi >nul 2>&1
if %errorlevel% neq 0 (
    echo WARNING: GPU not detected, using CPU
    set OLLAMA_NUM_GPU=0
) else (
    echo GPU detected, enabling acceleration
    set OLLAMA_NUM_GPU=1
)

REM Ollama baÅŸlat
ollama serve

pause
```

**AdÄ±m 3: Fallback MekanizmasÄ±**

```typescript
// llamaClient.ts
async generate(request: LlamaGenerateRequest): Promise<LlamaGenerateResponse> {
  try {
    // GPU ile dene
    return await this.generateWithGPU(request);
  } catch (error) {
    console.warn('âš ï¸ GPU failed, falling back to CPU');
    
    // CPU moduna geÃ§
    process.env.OLLAMA_NUM_GPU = '0';
    return await this.generateWithCPU(request);
  }
}
```

**AdÄ±m 4: GPU Bellek MonitÃ¶rÃ¼**

```typescript
// main.ts
import { exec } from 'child_process';

async function checkGPUMemory(): Promise<number> {
  return new Promise((resolve) => {
    exec('nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits', 
      (error, stdout) => {
        if (error) {
          resolve(0); // GPU yok
        } else {
          resolve(parseInt(stdout.trim()));
        }
      }
    );
  });
}

// Periyodik kontrol
setInterval(async () => {
  const memoryUsed = await checkGPUMemory();
  if (memoryUsed > 6000) { // 6GB Ã¼stÃ¼
    console.warn('âš ï¸ GPU memory high:', memoryUsed, 'MB');
  }
}, 30000); // Her 30 saniyede
```

#### Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Mod | Ä°lk YanÄ±t | Sonraki YanÄ±tlar | Bellek |
|-----|-----------|------------------|---------|
| **CPU (q4_k_m)** | ~5-8s | ~3-5s | ~2GB RAM |
| **GPU (q4_k_m)** | ~2-3s | ~0.5-1s | ~2.5GB VRAM |
| **GPU (q8_0)** | ~3-4s | ~1-2s | ~4GB VRAM |
| **GPU warm-up** | ~0.3s | ~0.3s | - |

#### Ã–nerilen YapÄ±landÄ±rma

**GeliÅŸtiriciler iÃ§in (GPU varsa):**
```bash
LLAMA_MODEL=gemma2:2b  # Fast 2B model
OLLAMA_NUM_GPU=1
GPU_WARMUP=true
MAX_CONTEXT_LENGTH=15000
```

**KullanÄ±cÄ±lar iÃ§in (GPU yoksa):**
```bash
LLAMA_MODEL=gemma3:4b-it-qat
OLLAMA_NUM_GPU=0  # CPU modu
GPU_WARMUP=false
MAX_CONTEXT_LENGTH=8000  # Daha kÄ±sa context
```

### GPU Bellek YÃ¶netimi

#### Otomatik Bellek Temizleme

Uygulama GPU belleÄŸini otomatik olarak izler ve gerektiÄŸinde temizler:

```typescript
// main.ts - Otomatik baÅŸlatÄ±lÄ±r
startGPUMemoryMonitor(
  30000,  // Her 30 saniyede kontrol
  6000,   // 6GB eÅŸik deÄŸeri
  true    // Otomatik temizleme aktif
);
```

**NasÄ±l Ã‡alÄ±ÅŸÄ±r:**
1. Her 30 saniyede GPU bellek kullanÄ±mÄ± kontrol edilir
2. KullanÄ±m 6GB'Ä± aÅŸarsa otomatik temizlik baÅŸlar
3. Ollama modelleri GPU'dan kaldÄ±rÄ±lÄ±r
4. Bellek serbest bÄ±rakÄ±lÄ±r
5. Sonraki sorguda model tekrar yÃ¼klenir

**Log Ã‡Ä±ktÄ±sÄ±:**
```
ğŸ“Š GPU Memory: 5200MB / 8192MB (63.5% used, 2992MB free)
âš ï¸ GPU memory HIGH: 6400MB / 8192MB (threshold: 6000MB)
ğŸ§¹ Triggering automatic GPU cleanup...
âœ… Unloaded model: gemma2:2b
ğŸ§¹ GPU Cleanup: 6400MB â†’ 450MB (freed: 5950MB)
âœ… Cleanup successful! Freed 5950MB
```

#### Manuel Bellek Temizleme

Settings sayfasÄ±ndan manuel olarak temizleyebilirsiniz:

1. **Settings** â†’ **AI & Performans AyarlarÄ±**
2. **GPU BelleÄŸini Temizle** butonuna tÄ±klayÄ±n
3. Onay mesajÄ± ile kaÃ§ MB serbest bÄ±rakÄ±ldÄ±ÄŸÄ± gÃ¶sterilir

**IPC KullanÄ±mÄ±:**
```typescript
// Renderer process'ten
const result = await window.electron.ipcRenderer.invoke('cleanup-gpu-memory');
console.log(`Freed ${result.freedMemoryMB}MB`);
```

#### Bellek EÅŸik AyarlarÄ±

`.env` dosyasÄ±nda yapÄ±landÄ±rma:

```bash
# GPU bellek eÅŸik deÄŸeri (MB)
GPU_MEMORY_THRESHOLD=6000

# Otomatik temizleme (true/false)
GPU_AUTO_CLEANUP=true
```

**Ã–nerilen EÅŸik DeÄŸerleri:**
- 4GB GPU: 3000MB
- 6GB GPU: 4500MB
- 8GB GPU: 6000MB
- 12GB+ GPU: 8000MB

#### Bellek Optimizasyon Ä°puÃ§larÄ±

**1. Model SeÃ§imi**
```bash
# DÃ¼ÅŸÃ¼k bellek (2.5GB)
LLAMA_MODEL=gemma2:2b  # Fast 2B model

# Orta bellek (4GB)
LLAMA_MODEL=gemma2:2b  # Fast 2B model
```

**2. Context Length AyarÄ±**
Daha kÄ±sa context = daha az bellek:
```bash
MAX_CONTEXT_LENGTH=8000   # ~500MB
MAX_CONTEXT_LENGTH=15000  # ~1GB
MAX_CONTEXT_LENGTH=32000  # ~2GB
```

**3. Keep-Alive AyarÄ±**
Model GPU'da ne kadar kalacak:
```bash
# Ollama config
OLLAMA_KEEP_ALIVE=5m  # 5 dakika sonra otomatik kaldÄ±r
OLLAMA_KEEP_ALIVE=0   # Hemen kaldÄ±r (her seferinde yeniden yÃ¼kler)
```

**4. Batch Size Azaltma**
AynÄ± anda iÅŸlenen sorgu sayÄ±sÄ±nÄ± sÄ±nÄ±rlayÄ±n:
```typescript
// chatController.ts
const maxConcurrentRequests = 1;  // GPU belleÄŸi sÄ±nÄ±rlÄ±ysa
```

## ğŸ› ï¸ Development

### File Structure

```
src/main/ai/
â”œâ”€â”€ chatController.ts      # Main orchestrator
â”œâ”€â”€ llamaClient.ts         # Llama API wrapper
â”œâ”€â”€ embedClient.ts         # BGE-M3 wrapper
â”œâ”€â”€ retrievalClient.ts     # Vector DB client
â”œâ”€â”€ numericExtractor.ts    # Deterministic extraction
â””â”€â”€ aggregator.ts          # Statistical aggregation

src/renderer/src/components/ChatBot/
â””â”€â”€ ChatBot.tsx            # React UI

tests/
â”œâ”€â”€ numericExtractor.test.ts
â””â”€â”€ aggregator.test.ts
```

### Extending the System

#### Add New Extraction Patterns

Edit `numericExtractor.ts`:

```typescript
export function extractCustomData(text: string) {
  // Add your pattern
}
```

#### Add New Aggregation Metrics

Edit `aggregator.ts`:

```typescript
export function aggregateCustomMetric(amounts: ExtractedAmount[]) {
  // Add your metric
}
```

## ğŸ”„ Switching Models

You can easily switch between models:

### Option 1: Environment Variable (Recommended)

Edit `.env`:
```bash
LLAMA_MODEL=gemma2:2b
```

Available models:
- `llama3.2:3b` - Lightweight, fast
- `gemma2:2b` - Fast & Efficient (Recommended) â­
- `qwen2.5:7b` - Best overall performance

### Option 2: Runtime Selection

The app automatically detects your `LLAMA_MODEL` environment variable. Just restart the app after changing it.

### Testing Different Models

```bash
# Test Gemma2 2b (Recommended)
ollama run gemma2:2b
# Type some Turkish text to test

# Test Qwen 2.5:7b
ollama run qwen2.5:7b
# Type some Turkish text to test

# Test Llama 3.2:3b
ollama run llama3.2:3b
# Type some Turkish text to test
```

Compare performance and choose what works best for your needs!

## ğŸ’¡ Document Assistant Mode - Usage Guide

### How to Use

1. **Upload Documents**: Process your documents (PDF, DOCX, Excel, etc.) through the app
2. **Switch to Document Mode**: Click "DokÃ¼man AsistanÄ±" button in chatbot
3. **Configure Options** (optional):
   - **Otomatik Hesaplama**: Enable automatic numeric calculations
   - **Ham Veri GÃ¶ster**: Show raw extracted values
   - **Max Referans**: Set maximum number of references (1-20)
4. **Ask Questions**: Query your documents naturally in Turkish

### System Prompt Features

The Document Assistant uses a production-ready system prompt with:

- **Retrieval-First Logic**: Searches documents using keyword â†’ partial â†’ n-gram â†’ semantic matching
- **Deterministic Extraction**: Extracts numbers using Turkish locale-aware regex (1.234,56 TL)
- **Structured Output**: Returns answer + `__meta__` JSON with references and statistics
- **Anomaly Detection**: Flags contradictions and data quality issues
- **Low Hallucination**: Temperature 0.15 for deterministic, fact-based responses

### Example Queries

#### Financial Queries (Document Mode)

- "Excel dosyalarÄ±ndaki toplam maaÅŸ nedir?" â†’ Extracts numbers, computes sum
- "Faturalardaki ortalama tutar nedir?" â†’ Returns avg with references
- "En yÃ¼ksek fatura tutarÄ± hangi dosyada?" â†’ Finds max value with source

#### Document Search

- "Employee Sample Data dosyasÄ±nda kaÃ§ Ã§alÄ±ÅŸan var?"
- "Hangi dokÃ¼manlarda 'invoice' kelimesi geÃ§iyor?"
- "PDF'lerdeki toplam sayfa sayÄ±sÄ± nedir?"

#### Structured Data

- "Excel'deki tÃ¼m departmanlarÄ± listele"
- "PowerPoint sunumlarÄ±ndaki baÅŸlÄ±klarÄ± gÃ¶ster"

### Response Format

Document Assistant returns two parts:

1. **Human-Readable Answer**: Natural Turkish text (1-3 sentences)
2. **__meta__ JSON Block**: Machine-parseable metadata

Example:
```
Toplam 3 faturada 45.600,50 TRY bulundu.

__meta__:
{
  "language": "tr",
  "query": "Fatura toplamÄ± nedir?",
  "foundReferences": [
    {
      "section_id": "excel_123_0_0_1",
      "document_id": "excel_123",
      "filename": "invoices.xlsx",
      "excerpt": "Toplam: 15.200,00 TL",
      "relevanceScore": 0.92,
      "page": 1
    }
  ],
  "numericValues": [
    {
      "section_id": "excel_123_0_0_1",
      "rawValue": "15.200,00 TL",
      "parsedValue": 15200.0,
      "currency": "TRY"
    }
  ],
  "aggregatesSuggested": {
    "count": 3,
    "sum": 45600.5,
    "avg": 15200.17
  },
  "confidence": 0.89
}
```

## ğŸ“ Example Queries (Legacy Mode)

### Financial Queries

- "Son ay iÃ§indeki faturalarÄ±mÄ±n toplam tutarÄ± ne kadar?"
- "Ortalama fatura tutarÄ±m nedir?"
- "En yÃ¼ksek fatura tutarÄ± ne?"
- "KaÃ§ adet fatura var?"

### Date-Based Queries

- "Mart ayÄ±ndaki faturalarÄ± gÃ¶ster"
- "2024 yÄ±lÄ± toplam fatura tutarÄ±"
- "Son 7 gÃ¼ndeki faturalar"

### Invoice-Specific

- "FT-2024-001 numaralÄ± fatura tutarÄ± nedir?"
- "Tekrar eden fatura var mÄ±?"

## ğŸ¯ Roadmap

- [ ] Multi-language support (English, German)
- [ ] Currency conversion
- [ ] Export results to Excel
- [ ] Voice input
- [ ] Scheduled reports

## ğŸ“š References

- [BGE-M3 Model](https://huggingface.co/BAAI/bge-m3)
- [Llama 3.2](https://ollama.ai/library/llama3.2)
- [Ollama Documentation](https://ollama.ai/docs)
- [Supabase pgvector](https://supabase.com/docs/guides/ai/vector-columns)
- [Qdrant](https://qdrant.tech/documentation/)

## ğŸ“„ License

MIT License - see LICENSE file

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch (`feature/mistral-chatbot-rag`)
3. Write tests
4. Submit pull request

---

**Built with â¤ï¸ for accurate, local-first AI document analysis**

