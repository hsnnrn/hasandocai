# Mistral 7B Kurulum Rehberi

## YÃ¶ntem 1: Ollama (Ã–NERÄ°LEN) â­

### 1. Ollama'yÄ± Ä°ndir ve Kur

**Windows iÃ§in:**
1. https://ollama.ai/download adresine git
2. "Download for Windows" butonuna tÄ±kla
3. OllamaSetup.exe dosyasÄ±nÄ± indir ve Ã§alÄ±ÅŸtÄ±r
4. Kurulum tamamlandÄ±ktan sonra bilgisayarÄ± yeniden baÅŸlat (gerekirse)

**Veya PowerShell ile:**
```powershell
winget install Ollama.Ollama
```

### 2. Mistral Modelini Ä°ndir

Kurulum tamamlandÄ±ktan sonra:

```bash
# Mistral 7B modelini indir (yaklaÅŸÄ±k 4GB)
ollama pull mistral

# Ä°ndirme tamamlandÄ±ÄŸÄ±nda modeli Ã§alÄ±ÅŸtÄ±r
ollama run mistral
```

### 3. Test Et

```bash
# Yeni bir terminal aÃ§ ve test et
curl http://localhost:11434/api/tags
```

Ã‡Ä±ktÄ±:
```json
{
  "models": [
    {
      "name": "mistral:latest",
      ...
    }
  ]
}
```

### 4. ChatBot'ta Kullan

Mistral Ã§alÄ±ÅŸÄ±rken DocDataApp'i aÃ§:
```bash
npm run dev
```

AI Chat sayfasÄ±na git ve saÄŸ Ã¼stte Mistral gÃ¶stergesinin yeÅŸil olduÄŸunu kontrol et! ğŸŸ¢

---

## YÃ¶ntem 2: Docker + Text Generation Inference

```bash
docker pull ghcr.io/huggingface/text-generation-inference:latest

docker run -p 11434:80 \
  -v $HOME/.cache/huggingface:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id mistralai/Mistral-7B-Instruct-v0.2
```

---

## YÃ¶ntem 3: llama.cpp (CPU iÃ§in)

### 1. GGUF Model Ä°ndir
```bash
# Quantized model (4-bit)
curl -L https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf -o mistral-7b.gguf
```

### 2. llama.cpp Kur
```bash
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
mkdir build
cd build
cmake ..
cmake --build . --config Release
```

### 3. Ã‡alÄ±ÅŸtÄ±r
```bash
./llama-server -m ../mistral-7b.gguf --port 11434
```

---

## Sorun Giderme

### Ollama baÅŸlamÄ±yor
```bash
# Servis durumunu kontrol et
ollama serve

# Port kullanÄ±mda mÄ±?
netstat -ano | findstr :11434
```

### Model indirme yavaÅŸ
```bash
# Ollama'nÄ±n kullandÄ±ÄŸÄ± mirror'Ä± deÄŸiÅŸtir
set OLLAMA_HOST=https://ollama.ai
ollama pull mistral
```

### GPU kullanÄ±lmÄ±yor
```bash
# CUDA versiyonunu kontrol et
nvidia-smi

# Ollama GPU kullanÄ±mÄ±nÄ± kontrol et
ollama run mistral
# Ã‡Ä±ktÄ±da "Using GPU" yazmalÄ±
```

---

## Model Bilgileri

| Ã–zellik | DeÄŸer |
|---------|-------|
| Model | Mistral 7B Instruct v0.2 |
| Boyut | ~4GB (quantized) |
| Context | 8192 tokens |
| Dil | TÃ¼rkÃ§e dahil Ã§ok dilli |
| Lisans | Apache 2.0 |

---

## Alternatif Modeller

Daha hafif alternatifler:

```bash
# Mistral 7B Q4 (daha kÃ¼Ã§Ã¼k)
ollama pull mistral:7b-instruct-q4_0

# Llama 3 8B (alternatif)
ollama pull llama3

# Phi-2 (Ã§ok hafif, CPU iÃ§in)
ollama pull phi
```

---

## Performans Ä°puÃ§larÄ±

### GPU KullanÄ±mÄ±
```bash
# NVIDIA GPU varsa otomatik kullanÄ±lÄ±r
# AMD GPU iÃ§in ROCm gerekir
```

### RAM Gereksinimleri
- Minimum: 8GB RAM
- Ã–nerilen: 16GB+ RAM
- Q4 quantized: ~4GB RAM
- Full FP16: ~14GB RAM

### CPU-Only Mod
```bash
# Ollama CPU kullanÄ±mÄ±
set OLLAMA_NUM_GPU=0
ollama run mistral
```

---

## ChatBot Entegrasyonu

DocDataApp'te Mistral kullanÄ±mÄ±:

1. âœ… Ollama Ã§alÄ±ÅŸÄ±yor olmalÄ± (`http://localhost:11434`)
2. âœ… Model indirilmiÅŸ olmalÄ± (`ollama list`)
3. âœ… `.env` dosyasÄ±nda:
   ```
   MISTRAL_SERVER_URL=http://127.0.0.1:11434
   ```

HazÄ±rsÄ±nÄ±z! ğŸš€

