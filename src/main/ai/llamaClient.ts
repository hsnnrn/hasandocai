/**
 * Llama Client - Wrapper for Gemma3:4b inference via Ollama
 * 
 * Supports Ollama API format
 */

export interface LlamaGenerateRequest {
  instructions: string;
  context: string;
  maxTokens?: number;
  temperature?: number;
  stream?: boolean;
}

export interface LlamaGenerateResponse {
  text: string;
  usage: {
    promptTokens?: number;
    completionTokens?: number;
    totalTokens?: number;
  };
  latencyMs: number;
  model: string;
}

export interface LlamaConfig {
  serverUrl: string;
  model: string;
  timeout: number;
  maxRetries: number;
}

/**
 * Llama prompt templates for formatting tasks
 */
export const LLAMA_PROMPTS = {
  // General chat system prompt
  system: `## AMA√á
- Kullanƒ±cƒ±nƒ±n komutlarƒ±nƒ± hƒ±zlƒ±, a√ßƒ±k ve dilbilgisel olarak doƒüru T√ºrk√ße ile yerine getir.
- Gereksiz a√ßƒ±klama yapma, cevabƒ± √∂z ve doƒürudan ver.
- Hata yapmamak, hƒ±zla yanƒ±t vermek ve konudan sapmamak √∂nceliklidir.
- Yanƒ±t verirken doƒüal T√ºrk√ße dil kurallarƒ±na (yazƒ±m, noktalama, ek uyumu, c√ºmle yapƒ±sƒ±) dikkat et.
- Teknik a√ßƒ±klama gerekiyorsa, kƒ±sa ve mantƒ±ksal sƒ±rayla yap.
- Karma≈üƒ±k i≈ülemleri sadele≈ütir, gerekirse adƒ±m adƒ±m a√ßƒ±kla ama gereksiz tekrar yapma.
- Gerektiƒüinde kod, tablo veya madde i≈üaretleriyle d√ºzenli bi√ßimde yaz.
- ƒ∞ngilizce terimler gerekiyorsa, T√ºrk√ßesiyle birlikte parantez i√ßinde belirt (√∂rn. "vekt√∂r (vector)").

## DAVRANI≈û KURALLARI
1. Yanƒ±tlarƒ±n net, bi√ßimsel ve profesyonel olsun. Gereksiz kelime kullanma.
2. Her c√ºmle bitiminde nokta kullan. Noktalama hatasƒ± yapma.
3. Cevap verirken m√ºmk√ºn olduƒüunca kƒ±sa c√ºmlelerle anlat.
4. Eƒüer kullanƒ±cƒ± teknik bir ≈üey soruyorsa, sadece ilgili bilgiyi d√∂nd√ºr.
5. Yanƒ±tƒ±n tamamƒ±nƒ± **T√ºrk√ße** ver; yalnƒ±zca deƒüi≈üken, fonksiyon adƒ±, kod veya terim gerekiyorsa ƒ∞ngilizce olabilir.
6. Belirsiz sorularda, √∂nce kƒ±sa bir netle≈ütirme sorusu y√∂nelt.
7. C√ºmle yapƒ±sƒ±nƒ± doƒüal tut ‚Äî yapay √ßeviri hissi verme.
8. T√ºrk√ße yazƒ±mda **T√ºrk Dil Kurumu (TDK)** kurallarƒ±na en yakƒ±n bi√ßimi kullan.

`,

  // Document assistant system prompt - PROFESSIONAL EDITION v2.0
  documentAssistant: `üìÑ **Dok√ºman Asistanƒ± Modu Aktif**

Sen modern, sezgisel ve doƒüal konu≈üan bir yapay zek√¢sƒ±n.
Kullanƒ±cƒ±nƒ±n localStorage'da tuttuƒüu belgeleri (LOCAL_DOCS) tanƒ±r, i√ßeriklerini analiz eder ve sanki bu belgeler senin belleƒüindeymi≈ü gibi yanƒ±tlar verirsin.

---

üß© TEMEL KURALLAR

1. **Belgeleri tanƒ±:** Belgeler localStorage'dan JSON formatƒ±nda gelir.
   Her belge:
   {
     "documentId": "...",
     "filename": "...",
     "fileType": "PDF | Excel | Word",
     "textSections": [{ "id": "...", "content": "..." }]
   }

2. **Analiz yap:** Belgelerdeki \`content\` alanlarƒ±nƒ± doƒüal dilde yorumla.
   - Eƒüer kullanƒ±cƒ± "bu dosyada ne var" derse ‚Üí dosyanƒ±n amacƒ±nƒ±, √∂zetini, hangi verileri i√ßerdiƒüini √ßƒ±kar.
   - Eƒüer "fatura miktarƒ± ne" derse ‚Üí sayƒ±sal deƒüerleri, para birimlerini ve toplamlarƒ± yakalamaya √ßalƒ±≈ü.
   - Eƒüer bilgi yoksa "Bu dosyada fatura tutarƒ±na dair bir bilgi bulamadƒ±m." de.
   - Belgede ilgili sayfayƒ± veya b√∂l√ºm√º bulduƒüunda, bilgiyi √∂zetleyip anlamlƒ± hale getir (sadece kopyalama deƒüil).

3. **Yanƒ±t bi√ßimi:**
   - Doƒüal, a√ßƒ±klayƒ±cƒ± ve kƒ±sa c√ºmlelerle konu≈ü.
   - Gereksiz "kaynak listeleri" veya "benzerlik oranlarƒ±" g√∂sterme.
   - Belgelerdeki verileri sanki "okuyup anlamƒ±≈üsƒ±n" gibi √∂zetle.
   - Kullanƒ±cƒ± sormadan dosya adlarƒ±nƒ± veya sayfa numaralarƒ±nƒ± tekrarlama.
   - Belgelerde bilgi eksikse kullanƒ±cƒ±yƒ± y√∂nlendir:
     "Bu belge fatura yapƒ±sƒ±nda ama tutar alanƒ± bo≈ü olabilir, istersen kontrol edebilirim." gibi.

4. **Davranƒ±≈ü stili:**
   - Ger√ßek bir AI gibi konu≈ü: akƒ±≈ükan, baƒülamƒ± takip eden, "ne anladƒ±ƒüƒ±nƒ±" a√ßƒ±klayan.
   - Teknik detaylarƒ± gerektiƒüinde sade bi√ßimde √∂zetle.
   - Kullanƒ±cƒ±nƒ±n √∂nceki sorularƒ±nƒ± hatƒ±rla (√∂rneƒüin "bu dosyada ne var?" ‚Üí sonra "fatura miktarƒ± ne?" dendiƒüinde baƒülamƒ± kaybetme).

---

√ñNCELƒ∞K & ARAMA AKI≈ûI (kesin sƒ±ralama):

1. **Dosya adƒ± (tam/kƒ±smi)** verilmi≈üse ‚Üí O dosyayƒ± ANINDA tespit et, SADECE o dosyayƒ± ara
   - "Invoice-13TVEI4D" ‚Üí "Invoice-13TVEI4D-0002.docx" E≈ûLE≈ûMEK ZORUNDA
   - B√ºy√ºk/k√º√ß√ºk harf √∂nemsiz, uzantƒ±yƒ± (.pdf, .docx) g√∂rmezden gel
   - Dosya yoksa: "Dosya bulunamadƒ±" bildir

2. **Dosya adƒ± yoksa** ‚Üí Metadata (filename, filetype, sheetName, pageNumber) ‚Üí ƒ∞√ßerik ‚Üí Fuzzy

3. **Excel/CSV** sheet'lerini ayrƒ± belge gibi ele al

ƒ∞√áERƒ∞K ARAMA:
- Chunk: ilgili 20 i√ßerik i√ßinde arama yap
- Fuzzy: T√ºrk√ße e≈üanlamlƒ± (fatura‚Üîinvoice)
- Excel: satƒ±r/s√ºtun bazlƒ± sorgu
- Sayƒ±sal, parasal veya tablo benzeri ifadeleri tanƒ±mlamaya √∂ncelik ver

---

üîç CEVAP AKI≈ûI √ñRNEKLERƒ∞

**Kullanƒ±cƒ±:** "Hangi belgeler var?"
**Sen:** "3 belge mevcut: sample-invoice.pdf, Employee Data.xlsx ve Contract.docx. Hangisini inceleyeyim?"

**Kullanƒ±cƒ±:** "sample-invoice.pdf dosyasƒ±nda ne var?"
**Sen:** "Bu dosya bir fatura √∂rneƒüi. ƒ∞√ßinde CPB Software GmbH ve Musterkunde AG'ye ait m√º≈üteri bilgileri var."

**Kullanƒ±cƒ±:** "Fatura miktarƒ± ne?"
**Sen:** "Faturada belirtilen tutar 1.000,00 EUR. Ancak belgeye g√∂re KDV bilgisi eksik olabilir."

**Kullanƒ±cƒ±:** "Toplam ka√ß dok√ºman y√ºkledim?"
**Sen:** "≈ûu anda localStorage'da 3 dok√ºman kayƒ±tlƒ±. ƒ∞stersen her birini √∂zetleyebilirim."

---

‚öôÔ∏è OPTƒ∞Mƒ∞ZASYON
- Belgelerdeki textSections sayƒ±sƒ± fazla ise, analiz ederken sadece **ilgili 20 i√ßerik** i√ßinde arama yap.
- Sayƒ±sal, parasal veya tablo benzeri ifadeleri tanƒ±mlamaya √∂ncelik ver.
- Modelin hafƒ±zasƒ±nda belgeleri √∂zet olarak sakla, tekrar tekrar sorgulama.
- Yanƒ±t s√ºresini kƒ±saltmak i√ßin semantik √∂zetleme kullan.
- Hƒ±z optimizasyonu i√ßin kƒ±sa cevaplar √ºret; kullanƒ±cƒ± isterse detaylandƒ±r.

---

√ñZEL KURALLAR:
- **Fatura**: invoice_no, invoice_date (YYYY-MM-DD), total_amount (+ currency), seller, buyer
- **Excel**: s√ºtun adlarƒ±, satƒ±r sayƒ±sƒ±, departman √∂zeti
- **PDF**: sayfa numaralƒ± snippet
- **Word**: ba≈ülƒ±k + paragraf √∂zeti

BULUNAMADI DURUMU:
"Belgelerinizde bu bilgiye rastlamadƒ±m. Deneyebileceƒüiniz alternatifler: 1) daha spesifik anahtar kelime, 2) dosya adƒ±nƒ± yazƒ±n (√∂rn. Invoice-13TVEI4D-0002.docx), 3) arama kapsamƒ±nƒ± geni≈ületin."

FORMAT YASAƒûI:
‚ùå Numaralƒ± liste (1., 2., 3.)
‚ùå Markdown bold (**)
‚ùå Gereksiz ba≈ülƒ±k
‚ùå Uydurma yasak!

DOƒûRU √ñRNEKLER:
‚úÖ "Fatura tutarƒ± 2.458,30 EUR"
‚úÖ "Employee dosyasƒ±nda 1000 √ßalƒ±≈üan (Sheet: Employees, S√ºtunlar: Name, Dept, Salary)"
‚úÖ "En y√ºksek maa≈ü 185.000 USD - Jennifer Thomas (Sheet: Employees, Satƒ±r: 42)"

√úSLUP:
- Nazik, kƒ±sa, profesyonel
- Doƒüal ve akƒ±≈ükan T√ºrk√ße
- Belirsizlik durumunda: "muhtemelen" kullan
- Tarih: ISO (YYYY-MM-DD), para birimi a√ßƒ±k

---

üß© KISACA:
Sen artƒ±k belgeleri sadece "okuyan" deƒüil, "anlayan" bir dok√ºman asistanƒ±sƒ±n.
Soru ne olursa olsun ‚Äî belge i√ßeriƒüine dayalƒ±, doƒüal, g√ºvenilir ve sezgisel bir yanƒ±t ver.

`,

  userTemplate: (snippets: string, stats: string, provenance: string) => `Baƒülam:
${snippets}

ƒ∞statistikler:
${stats}

Kaynaklar:
${provenance}

Bu bilgilere dayanarak kullanƒ±cƒ± sorusuna kapsamlƒ±, doƒüal bir T√ºrk√ße yanƒ±t ver.`,

  noDataTemplate: (query: string) => `Kullanƒ±cƒ± sordu: "${query}"

ƒ∞lgili veri bulunamadƒ±. Yardƒ±mcƒ± bir yanƒ±t ver.`,
};

/**
 * Deep analysis prompts
 */
export const DEEP_ANALYSIS_PROMPTS = {
  generateSystem: `Sen akƒ±llƒ± bir dok√ºman analiz asistanƒ±sƒ±n. Saƒülanan verilere dayanarak kapsamlƒ± ve doƒüal T√ºrk√ße yanƒ±tlar ver.`,

  generateUser: (snippets: string[], computedStats: any, flags: any) => {
    const snippetsText = snippets.map((s, i) => `[${i + 1}] ${s}`).join('\n\n');
    const statsJson = JSON.stringify(computedStats, null, 2);
    
    return `Baƒülam:
${snippetsText}

ƒ∞statistikler:
${statsJson}

JSON formatƒ±nda d√∂nd√ºr: { "answer": string, "provenance": [], "followUp": string }`;
  },
};

export class LlamaClient {
  private config: LlamaConfig;

  constructor(config: Partial<LlamaConfig> = {}) {
    this.config = {
      serverUrl: config.serverUrl || process.env.LLAMA_SERVER_URL || 'http://127.0.0.1:11434',
      model: config.model || process.env.LLAMA_MODEL || 'gemma3:4b-it-qat', // Default: Gemma 3 4B - Fast and accurate
      timeout: config.timeout || 180000, // 180 saniye (3 dakika - uzun promptlar i√ßin)
      maxRetries: config.maxRetries || 1, // 1 retry
    };
  }

  /**
   * Check if Llama server is available
   */
  async healthCheck(): Promise<boolean> {
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000);

      const response = await fetch(`${this.config.serverUrl}/api/tags`, {
        method: 'GET',
        signal: controller.signal,
      });

      clearTimeout(timeoutId);
      return response.ok;
    } catch (error) {
      console.warn('Llama health check failed:', error);
      return false;
    }
  }

  /**
   * Generate text with Llama (Ollama format)
   */
  async generate(request: LlamaGenerateRequest): Promise<LlamaGenerateResponse> {
    const startTime = Date.now();

    const fullPrompt = `${LLAMA_PROMPTS.system}\n\n${request.instructions}\n\n${request.context}`;

    const payload = {
      model: this.config.model,
      prompt: fullPrompt,
      stream: request.stream || false,
      options: {
        temperature: request.temperature || 0.7, // Gemma2 i√ßin optimal: 0.7-0.8
        num_predict: request.maxTokens || 512,
        top_p: 0.9,
        top_k: 40,
        repeat_penalty: 1.1
      },
    };

    let lastError: Error | null = null;

    for (let attempt = 0; attempt <= this.config.maxRetries; attempt++) {
      try {
        const axios = require('axios');

        const response = await axios.post(
          `${this.config.serverUrl}/api/generate`,
          payload,
          {
            timeout: this.config.timeout,
            headers: {
              'Content-Type': 'application/json',
            },
          }
        );

        const data = response.data;
        const latencyMs = Date.now() - startTime;

        return {
          text: data.response || '',
          usage: {
            promptTokens: data.prompt_eval_count,
            completionTokens: data.eval_count,
            totalTokens: (data.prompt_eval_count || 0) + (data.eval_count || 0),
          },
          latencyMs,
          model: data.model || this.config.model,
        };
      } catch (error) {
        lastError = error as Error;

        if (attempt < this.config.maxRetries) {
          await new Promise(resolve => setTimeout(resolve, Math.pow(2, attempt) * 1000));
        }
      }
    }

    throw new Error(
      `Llama generate failed after ${this.config.maxRetries + 1} attempts: ${lastError?.message}`
    );
  }

  /**
   * Format a complete answer with stats and provenance
   */
  async formatAnswer(
    snippets: string[],
    computedStats: any,
    provenance: any[]
  ): Promise<LlamaGenerateResponse> {
    const snippetsText = snippets.join('\n\n---\n\n');
    const statsJson = JSON.stringify(computedStats, null, 2);
    const provenanceJson = JSON.stringify(provenance, null, 2);

    const userPrompt = LLAMA_PROMPTS.userTemplate(snippetsText, statsJson, provenanceJson);

    return await this.generate({
      instructions: userPrompt,
      context: '',
      maxTokens: 4000,
      temperature: 0.7, // Gemma2 i√ßin optimal
    });
  }

  /**
   * Generate "no data" message
   */
  async generateNoDataResponse(query: string): Promise<LlamaGenerateResponse> {
    const prompt = LLAMA_PROMPTS.noDataTemplate(query);

    return await this.generate({
      instructions: prompt,
      context: '',
      maxTokens: 2000,
      temperature: 0.7, // Gemma2 i√ßin optimal
    });
  }

  /**
   * Document-aware chat response with LOCAL_DOCS integration
   * OPTIMIZED: Follows new system prompt format with natural conversation
   */
  async documentAwareChat(
    query: string,
    localDocs: any[],
    options: {
      compute?: boolean;
      showRaw?: boolean;
      maxRefs?: number;
      locale?: string;
    } = {},
    conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = []
  ): Promise<LlamaGenerateResponse> {
    const startTime = Date.now();
    
    // Build conversation context
    let conversationContext = '';
    const recentHistory = conversationHistory.slice(-3);
    if (recentHistory.length > 0) {
      conversationContext = '√ñnceki konu≈üma:\n';
      for (const msg of recentHistory) {
        conversationContext += `${msg.role === 'user' ? 'Kullanƒ±cƒ±' : 'Asistan'}: ${msg.content}\n`;
      }
      conversationContext += '\n';
    }

    // Build LOCAL_DOCS format (optimized - only relevant sections)
    // Limit to 10000 chars total as per user's spec
    const localDocsData = localDocs.map((doc) => ({
      documentId: doc.documentId,
      filename: doc.filename,
      fileType: doc.fileType,
      textSections: doc.textSections?.slice(0, 20).map((s: any) => ({
        id: s.id,
        content: s.content.substring(0, 400), // Limit to 400 chars per section for optimization
        contentLength: s.contentLength
      })) || []
    }));

    // Stringify and limit to 10000 chars
    const localDocsJson = JSON.stringify(localDocsData, null, 2).slice(0, 10000);

    // Build user prompt following the new format
    const userPrompt = `${query}

LOCAL_DOCS:
${localDocsJson}

${conversationContext ? `GE√áMƒ∞≈û KONU≈ûMA:\n${conversationContext}` : ''}`;

    // Use documentAssistant system prompt
    const fullPrompt = `${LLAMA_PROMPTS.documentAssistant}\n\n${userPrompt}`;

    const payload = {
      model: this.config.model,
      prompt: fullPrompt,
      stream: false,
      options: {
        temperature: 0.15, // Low for accuracy but slightly higher for natural conversation
        num_predict: 400, // Increased for more detailed natural responses
        top_p: 0.9,
        top_k: 40,
        repeat_penalty: 1.1
      },
    };

    try {
      const axios = require('axios');

      const response = await axios.post(
        `${this.config.serverUrl}/api/generate`,
        payload,
        {
          timeout: this.config.timeout,
          headers: {
            'Content-Type': 'application/json',
          },
        }
      );

      const data = response.data;

      return {
        text: data.response || '',
        usage: {
          promptTokens: data.prompt_eval_count,
          completionTokens: data.eval_count,
          totalTokens: (data.prompt_eval_count || 0) + (data.eval_count || 0),
        },
        latencyMs: Date.now() - startTime,
        model: data.model || this.config.model,
      };
    } catch (error) {
      throw new Error(`Document-aware chat failed: ${error instanceof Error ? error.message : 'Unknown'}`);
    }
  }

  /**
   * Simple chat for casual conversation and direct queries
   * Optimized for short, focused responses
   */
  async simpleChat(
    prompt: string,
    conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = []
  ): Promise<{ text: string; model: string }> {
    
    console.log('üîß simpleChat called with:', {
      promptLength: prompt.length,
      historyLength: conversationHistory.length,
      history: conversationHistory.slice(-3).map(h => ({ 
        role: h.role, 
        content: h.content.substring(0, 50) + '...' 
      }))
    });
    
    // Build minimal conversation context with anti-thinking system prompt
    let conversationContext = `${LLAMA_PROMPTS.system}\n\n`;
    
    // Add last 3 messages from history
    const recentHistory = conversationHistory.slice(-3);
    if (recentHistory.length > 0) {
      conversationContext += '√ñnceki konu≈üma:\n';
      for (const msg of recentHistory) {
        conversationContext += `${msg.role === 'user' ? 'Kullanƒ±cƒ±' : 'Asistan'}: ${msg.content}\n`;
      }
      conversationContext += '\n';
      console.log('‚úÖ Added conversation history to context:', recentHistory.length, 'messages');
    } else {
      console.log('‚ö†Ô∏è No conversation history available');
    }
    
    conversationContext += `Kullanƒ±cƒ±: ${prompt}\nAsistan:`;
    
    const payload = {
      model: this.config.model,
      prompt: conversationContext,
      stream: false,
      options: {
        temperature: 0.1, // Low for accuracy
        num_predict: 800, // Higher limit for detailed summaries and document analysis
        top_p: 0.9,
        top_k: 40,
        repeat_penalty: 1.15
      },
    };
    
    const startTime = Date.now();
    
    try {
      // Use axios instead of fetch for better reliability in Electron
      const axios = require('axios');
      
      const response = await axios.post(
        `${this.config.serverUrl}/api/generate`,
        payload,
        {
          timeout: this.config.timeout,
          headers: {
            'Content-Type': 'application/json',
          },
        }
      );

      const data = response.data;

      return {
        text: data.response || '',
        model: data.model || this.config.model,
      };
    } catch (error) {
      console.error('‚ùå Ollama request failed:', error);
      throw new Error(`Simple chat failed: ${error instanceof Error ? error.message : 'Unknown'}`);
    }
  }

  /**
   * Pure chat response with conversation history
   */
  async chatResponse(
    query: string, 
    conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }> = []
  ): Promise<LlamaGenerateResponse> {
    // Reasonable limit for Gemma2 2B (8K token context)
    const maxQueryLength = 4000; // Max 4000 characters (~3-4K tokens)
    const truncatedQuery = query.length > maxQueryLength 
      ? query.substring(0, maxQueryLength) + '\n\n[Not: Mesaj √ßok uzun olduƒüu i√ßin burada kesildi]'
      : query;

    // Build conversation context with history and anti-thinking prompt
    let conversationContext = `${LLAMA_PROMPTS.system}\n\n√ñNEMLƒ∞: CEVABINI SADECE T√úRK√áE VER. ƒ∞ngilizce kelime kullanma.\n\n`;
    
    // Add last 5 messages from history (to stay within context limits)
    const recentHistory = conversationHistory.slice(-5);
    if (recentHistory.length > 0) {
      conversationContext += '√ñnceki konu≈üma:\n';
      for (const msg of recentHistory) {
        conversationContext += `${msg.role === 'user' ? 'Kullanƒ±cƒ±' : 'Asistan'}: ${msg.content}\n`;
      }
      conversationContext += '\n';
    }
    
    conversationContext += `Kullanƒ±cƒ±: ${truncatedQuery}\nAsistan (T√ºrk√ße):`;
    const fullPrompt = conversationContext;
    
    const payload = {
      model: this.config.model,
      prompt: fullPrompt,
      stream: false,
      options: {
        temperature: 0.7, // Gemma2 i√ßin optimal: 0.7-0.8
        num_predict: 300,
        top_p: 0.9,
        top_k: 40,
        repeat_penalty: 1.1
      },
    };

    console.log('üîß LlamaClient Config:', {
      serverUrl: this.config.serverUrl,
      model: this.config.model,
      timeout: this.config.timeout
    });

    const startTime = Date.now();

    try {
      const axios = require('axios');

      console.log('üì° Sending request to Ollama:', `${this.config.serverUrl}/api/generate`);
      console.log('üì¶ Payload:', { model: payload.model, promptLength: fullPrompt.length });

      const response = await axios.post(
        `${this.config.serverUrl}/api/generate`,
        payload,
        {
          timeout: this.config.timeout,
          headers: {
            'Content-Type': 'application/json',
          },
        }
      );

      const data = response.data;
      console.log('‚úÖ Ollama response received:', {
        model: data.model,
        responseLength: data.response?.length || 0
      });

      return {
        text: data.response || '',
        usage: {
          promptTokens: data.prompt_eval_count,
          completionTokens: data.eval_count,
          totalTokens: (data.prompt_eval_count || 0) + (data.eval_count || 0),
        },
        latencyMs: Date.now() - startTime,
        model: data.model || this.config.model,
      };
    } catch (error) {
      console.error('‚ùå LlamaClient chatResponse error:', error);
      throw new Error(`Chat failed: ${error instanceof Error ? error.message : 'Unknown'}`);
    }
  }

  /**
   * Analyze document content and extract key information
   */
  private static analyzeDocumentContent(content: string): {
    isInvoice: boolean;
    amounts: string[];
    dates: string[];
    invoiceNumber: string | null;
    company: string | null;
  } {
    const lowerContent = content.toLowerCase();
    const isInvoice = /invoice|fatura/.test(lowerContent);
    
    // Extract amounts (simple pattern)
    const amountMatches = content.match(/[\$‚Ç¨‚Ç∫¬£]\s*[\d,]+\.?\d*/g) || 
                         content.match(/[\d,]+\.?\d*\s*(?:USD|EUR|TRY|TL)/gi) || [];
    const amounts = [...new Set(amountMatches)].slice(0, 3); // Unique, max 3
    
    // Extract dates (simple patterns)
    const dateMatches = content.match(/\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4}/g) ||
                       content.match(/(?:January|February|March|April|May|June|July|August|September|October|November|December|Ocak|≈ûubat|Mart|Nisan|Mayƒ±s|Haziran|Temmuz|Aƒüustos|Eyl√ºl|Ekim|Kasƒ±m|Aralƒ±k)\s+\d{1,2},?\s+\d{4}/gi) || [];
    const dates = [...new Set(dateMatches)].slice(0, 2);
    
    // Extract invoice number
    const invoiceMatch = content.match(/(?:invoice|fatura)\s*(?:number|no|#|numara)[:\s]*([A-Z0-9\-]+)/i);
    const invoiceNumber = invoiceMatch ? invoiceMatch[1] : null;
    
    // Extract company name (basic heuristic)
    const lines = content.split('\n').filter(l => l.trim());
    const company = lines.find(line => 
      line.length > 3 && line.length < 50 && 
      /^[A-Z]/.test(line) && 
      !/^(invoice|fatura|bill|date|total|amount)/i.test(line)
    ) || null;
    
    return { isInvoice, amounts, dates, invoiceNumber, company };
  }

  /**
   * Generate formatted answer for deep analysis
   */
  async generateDeepAnswer(
    snippets: string[],
    computedStats: any,
    flags: any
  ): Promise<{
    answer: string;
    provenance: any[];
    followUp: string;
    displayFlags: any;
    latencyMs: number;
  }> {
    const startTime = Date.now();
    
    try {
      const userPrompt = DEEP_ANALYSIS_PROMPTS.generateUser(snippets, computedStats, flags);
      
      const response = await this.generate({
        instructions: DEEP_ANALYSIS_PROMPTS.generateSystem,
        context: userPrompt,
        maxTokens: 4000,
        temperature: 0.7, // Gemma2 i√ßin optimal
      });
      
      try {
        const parsed = JSON.parse(response.text);
        return {
          answer: parsed.answer || response.text,
          provenance: parsed.provenance || [],
          followUp: parsed.followUp || '',
          displayFlags: parsed.displayFlags || flags,
          latencyMs: response.latencyMs,
        };
      } catch (parseError) {
        return {
          answer: response.text,
          provenance: [],
          followUp: '',
          displayFlags: flags,
          latencyMs: response.latencyMs,
        };
      }
    } catch (error) {
      throw error;
    }
  }


  /**
   * Get fallback response when Llama is down
   * Intelligently responds based on available data
   */
  static getFallbackResponse(
    computedStats: any,
    provenance: any[]
  ): string {
    const { count, sum, average, median, currency } = computedStats;

    // If we have provenance but no stats, analyze and provide intelligent document information
    if (provenance && provenance.length > 0 && count === 0) {
      // Deep analysis mode - analyze all documents in detail
      const analyses = provenance.map(prov => ({
        filename: prov.filename,
        analysis: this.analyzeDocumentContent(prov.snippet + (prov.metadata?.content || ''))
      }));
      
      const invoices = analyses.filter(a => a.analysis.isInvoice);
      const nonInvoices = analyses.filter(a => !a.analysis.isInvoice);
      
      let response = `üß† **Derin Analiz Sonu√ßlarƒ±**\n\n`;
      response += `üìä **Genel Bakƒ±≈ü:** ${provenance.length} dok√ºman tarandƒ±\n`;
      response += `üìù **Faturalar:** ${invoices.length} adet\n`;
      response += `üìÑ **Diƒüer Dok√ºmanlar:** ${nonInvoices.length} adet\n\n`;
      
      if (invoices.length > 0) {
        response += `---\n\n`;
        response += `üíº **FATURA DETAYLARI:**\n\n`;
        
        invoices.forEach((item, index) => {
          response += `**${index + 1}. ${item.filename}**\n`;
          
          if (item.analysis.invoiceNumber) {
            response += `   üî¢ Fatura No: ${item.analysis.invoiceNumber}\n`;
          }
          if (item.analysis.company) {
            response += `   üè¢ ≈ûirket: ${item.analysis.company}\n`;
          }
          if (item.analysis.dates.length > 0) {
            response += `   üìÖ Tarih: ${item.analysis.dates[0]}\n`;
          }
          if (item.analysis.amounts.length > 0) {
            response += `   üí∞ Tutarlar: ${item.analysis.amounts.join(', ')}\n`;
            
            // Try to identify which is the total amount
            const totalMatch = item.analysis.amounts.find(amt => 
              /total|toplam/i.test(amt) || item.analysis.amounts.length === 1
            );
            if (totalMatch) {
              response += `   ‚úÖ Ana Tutar: **${totalMatch}**\n`;
            }
          }
          response += `\n`;
        });
        
        // Extract all unique amounts
        const allAmounts = invoices.flatMap(inv => inv.analysis.amounts);
        const uniqueAmounts = [...new Set(allAmounts)];
        
        if (uniqueAmounts.length > 0) {
          response += `---\n\n`;
          response += `üìà **Mali √ñzet:**\n`;
          response += `   ‚Ä¢ Toplam ${invoices.length} fatura\n`;
          response += `   ‚Ä¢ Tespit edilen tutarlar: ${uniqueAmounts.join(', ')}\n`;
          response += `   ‚Ä¢ Farklƒ± tutar sayƒ±sƒ±: ${uniqueAmounts.length}\n`;
        }
      }
      
      if (nonInvoices.length > 0) {
        response += `\n---\n\n`;
        response += `üìÑ **Dƒ∞ƒûER DOK√úMANLAR:**\n\n`;
        nonInvoices.forEach((item, index) => {
          response += `${index + 1}. **${item.filename}**\n`;
          response += `   T√ºr: Genel dok√ºman\n\n`;
        });
      }
      
      response += `---\n\n`;
      response += `üí° **Sonraki Adƒ±mlar:**\n`;
      response += `Daha detaylƒ± hesaplamalar i√ßin ≈üu sorularƒ± sorabilirsiniz:\n`;
      response += `   ‚Ä¢ "Fatura toplamƒ± nedir?" - T√ºm tutarlarƒ± topla\n`;
      response += `   ‚Ä¢ "En y√ºksek fatura hangisi?" - Kar≈üƒ±la≈ütƒ±rma yap\n`;
      response += `   ‚Ä¢ "Tarih sƒ±rasƒ±na g√∂re g√∂ster" - Kronolojik sƒ±ralama\n`;
      
      return response;
    }

    // If no data at all
    if (count === 0 && (!provenance || provenance.length === 0)) {
      return '‚ùå Bu sorgu i√ßin veri bulamadƒ±m. L√ºtfen daha fazla belge y√ºkleyin veya sorgunuzu farklƒ± kelimelerle ifade edin.';
    }

    // If we have numeric stats
    const currencyStr = currency || 'TRY';
    
    let response = `üìä **Fatura Analizi**\n\n`;
    response += `‚úÖ Toplam **${count} adet** fatura bulundu\n\n`;
    response += `üí∞ **Finansal √ñzet:**\n`;
    response += `   ‚Ä¢ Toplam tutar: **${sum.toLocaleString('tr-TR', { minimumFractionDigits: 2 })} ${currencyStr}**\n`;
    response += `   ‚Ä¢ Ortalama: ${average.toLocaleString('tr-TR', { minimumFractionDigits: 2 })} ${currencyStr}\n`;
    response += `   ‚Ä¢ Medyan: ${median.toLocaleString('tr-TR', { minimumFractionDigits: 2 })} ${currencyStr}\n`;
    response += `\nüìÑ **Kaynaklar:**\n`;

    provenance.slice(0, 5).forEach((prov, index) => {
      response += `   ${index + 1}. ${prov.filename}\n`;
    });

    response += `\n*ü§ñ Not: Bu yanƒ±t otomatik analiz ile olu≈üturuldu.*`;

    return response;
  }
}

