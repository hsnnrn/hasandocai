/**
 * Mock LLM Runner - Development/Testing
 * 
 * Llama.cpp olmadan test iÃ§in basit mock responses
 */

export interface LLMConfig {
  modelPath: string;
  maxTokens: number;
  temperature: number;
  timeout: number;
}

export interface GenerationOptions {
  maxTokens?: number;
  temperature?: number;
  timeout?: number;
}

export class MockLLMRunner {
  private config: LLMConfig;
  private isInitialized: boolean = false;

  constructor(modelPath: string) {
    this.config = {
      modelPath,
      maxTokens: 512,
      temperature: 0.7,
      timeout: 60000
    };
  }

  /**
   * Initialize mock LLM runner
   */
  async initialize(): Promise<void> {
    try {
      console.log('ğŸ”§ Initializing Mock LLM Runner...');
      this.isInitialized = true;
      console.log('âœ… Mock LLM Runner initialized');
    } catch (error) {
      console.error('âŒ Failed to initialize Mock LLM Runner:', error);
      throw error;
    }
  }

  /**
   * Generate mock response
   */
  async generateResponse(prompt: string, options: GenerationOptions = {}): Promise<string> {
    if (!this.isInitialized) {
      throw new Error('Mock LLM Runner not initialized');
    }

    try {
      console.log('ğŸ§  Mock: Generating LLM response...');
      
      // Simulate processing time
      await new Promise(resolve => setTimeout(resolve, 1000));
      
      // Generate mock response based on prompt
      const mockResponse = this.generateMockResponse(prompt);
      
      console.log('âœ… Mock: LLM response generated');
      return mockResponse;
    } catch (error) {
      console.error('âŒ Mock: Failed to generate response:', error);
      throw error;
    }
  }

  /**
   * Generate mock response based on prompt
   */
  private generateMockResponse(prompt: string): string {
    // Simple keyword-based responses
    const lowerPrompt = prompt.toLowerCase();
    
    if (lowerPrompt.includes('belge') || lowerPrompt.includes('document')) {
      return "Bu belge hakkÄ±nda size yardÄ±mcÄ± olabilirim. Belgenin iÃ§eriÄŸine dayanarak sorularÄ±nÄ±zÄ± yanÄ±tlayabilirim.";
    }
    
    if (lowerPrompt.includes('ne') || lowerPrompt.includes('what')) {
      return "Belgenizdeki bilgilere gÃ¶re, bu konu hakkÄ±nda detaylÄ± aÃ§Ä±klama yapabilirim. Hangi spesifik konuda bilgi almak istiyorsunuz?";
    }
    
    if (lowerPrompt.includes('nasÄ±l') || lowerPrompt.includes('how')) {
      return "Belgenizdeki adÄ±mlarÄ± ve sÃ¼reÃ§leri aÃ§Ä±klayabilirim. Hangi konuda yardÄ±ma ihtiyacÄ±nÄ±z var?";
    }
    
    if (lowerPrompt.includes('neden') || lowerPrompt.includes('why')) {
      return "Belgenizdeki bilgilere dayanarak neden-sonuÃ§ iliÅŸkilerini aÃ§Ä±klayabilirim.";
    }
    
    // Default response
    return "Belgenizdeki bilgilere dayanarak sorularÄ±nÄ±zÄ± yanÄ±tlayabilirim. Daha spesifik bir soru sorarsanÄ±z size daha detaylÄ± yardÄ±m edebilirim.";
  }

  /**
   * Check if LLM runner is ready
   */
  isReady(): boolean {
    return this.isInitialized;
  }

  /**
   * Get model info
   */
  getModelInfo(): { modelPath: string; isReady: boolean } {
    return {
      modelPath: this.config.modelPath,
      isReady: this.isInitialized
    };
  }

  /**
   * Cleanup resources
   */
  async cleanup(): Promise<void> {
    try {
      console.log('ğŸ§¹ Mock: Cleaning up LLM runner...');
      this.isInitialized = false;
      console.log('âœ… Mock: LLM runner cleanup completed');
    } catch (error) {
      console.error('âŒ Mock: Error during cleanup:', error);
    }
  }
}
