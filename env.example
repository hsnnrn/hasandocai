# Supabase OAuth Configuration
# Bu değerleri Supabase Dashboard'dan alın
SUPABASE_OAUTH_CLIENT_ID=your_client_id_here
SUPABASE_OAUTH_CLIENT_SECRET=your_client_secret_here

# Supabase Database Configuration
# Bu değerleri Supabase Project Settings > API'den alın
SUPABASE_ANON_KEY=your_supabase_anon_key_here

# OAuth Server Configuration
OAUTH_PORT=54321

# Redirect URI'lar (Supabase Dashboard'da tanımlanmalı)
REDIRECT_URI_LOCAL=http://localhost:54321/callback
REDIRECT_URI_CUSTOM=myapp://oauth/callback

# Keytar Service Configuration
SERVICE_NAME=DocDataApp

# Optional: Supabase Management API Base URL
SUPABASE_API_BASE_URL=https://api.supabase.com/v1

# Optional: OAuth Scopes (virgülle ayrılmış)
OAUTH_SCOPES=read:organizations,read:projects,read:api-keys,read:storage,read:functions

# Development Mode
NODE_ENV=development

# Optional: Debug Mode (OAuth flow için detaylı loglar)
DEBUG_OAUTH=true

# AI Model Configuration
# BGE-M3 Model Server (Python Flask)
BGE_MODEL_SERVER_URL=http://127.0.0.1:5000

# Llama Server (Ollama)
LLAMA_SERVER_URL=http://127.0.0.1:11434

# LLM Model Selection (recommended: llama3.2:3b for fast inference)
# Options: llama3.2:3b, gemma2:2b, qwen2.5:7b
LLAMA_MODEL=llama3.2:3b

# GPU Configuration
# GPU_MODE: auto, enabled, disabled
GPU_MODE=auto
# OLLAMA_NUM_GPU: 0 = CPU only, 1+ = GPU count
OLLAMA_NUM_GPU=1
# GPU Warm-up on app startup (recommended for faster first response)
GPU_WARMUP=true
# Max context length (lower = faster, higher = more context)
MAX_CONTEXT_LENGTH=15000
# GPU Memory Monitor (auto cleanup when threshold exceeded)
GPU_MEMORY_THRESHOLD=6000
GPU_AUTO_CLEANUP=true

# Deep Analysis Configuration
ENABLE_CRITIC=true
CRITIC_MODEL=llama
CRITIC_TIMEOUT=2000
DEEP_ANALYSIS_TIMEOUT=15000

# Optional: Escalate to stronger model if critic fails
# ESCALATE_MODEL=mixtral

# Vector Database Configuration
VECTOR_DB=local